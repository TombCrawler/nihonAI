{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMgdwnnJuQt2jLLLHiDDrLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TombCrawler/nihonAI/blob/main/TensorTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "referred"
      ],
      "metadata": {
        "id": "-sjlH-LzjC3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "cZCatou1E6M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "78YW6KByi_CL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.datasets import cifar10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "initialization of Tensors"
      ],
      "metadata": {
        "id": "9X3EHseUjovb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaler\n",
        "x = tf.constant(4, shape=(1,1), dtype=tf.float32)\n",
        "\n",
        "# Matrix, default 2 * 3 meaning shape=(2,3)\n",
        "x = tf.constant([[1,2,3], [4,5,6]])\n",
        "\n",
        "x = tf.ones((3,3))\n",
        "x = tf.zeros((2,3))\n",
        "x = tf.eye(3) # I for the indentity\n",
        "x = tf.random.normal((3,3), mean=0, stddev=1) # stddv for standard deviation\n",
        "x = tf.random.uniform((1,3), minval=0, maxval=1)\n",
        "x = tf.range(start=1, limit=10, delta=2) # delta is steps.\n",
        "x = tf.cast(x, dtype=tf.float64)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkihwZgNj7mN",
        "outputId": "2b8a1a6b-2369-45e0-801b-857b3595865a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([1. 3. 5. 7. 9.], shape=(5,), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematical Operations\n"
      ],
      "metadata": {
        "id": "DLzgK999jsTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([1,2,3])\n",
        "y = tf.constant([9,8,7])\n",
        "\n",
        "z = tf.add(x,y)\n",
        "# z = x + y\n",
        "\n",
        "z = tf.subtract(x, y)\n",
        "\n",
        "z = tf.divide(x, y)\n",
        "z = x/y\n",
        "\n",
        "z = tf.multiply(x, y)\n",
        "\n",
        "z = tf.tensordot(x, y, axes=1)\n",
        "z = tf.reduce_sum(x*y, axis=0)\n",
        "\n",
        "z = x ** 5\n",
        "\n",
        "x = tf.random.normal((2,3))\n",
        "y = tf.random.normal((3,4))\n",
        "z = tf.matmul(x, y)\n",
        "print(z)\n",
        "z = x @ y\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU130I9ew4d9",
        "outputId": "7919c1b6-183e-43d2-d94f-66d9af02b012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.0623768   1.777854   -0.15115848 -0.57498646]\n",
            " [-1.8350759   4.2908087  -0.4893546  -0.20726681]], shape=(2, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.0623768   1.777854   -0.15115848 -0.57498646]\n",
            " [-1.8350759   4.2908087  -0.4893546  -0.20726681]], shape=(2, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing"
      ],
      "metadata": {
        "id": "12BEgXErjxTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # <Vector>\n",
        "# x = tf.constant([6, 7, 8, 1, 3, 9, 2, 5])\n",
        "# print(x[:]) # Print all indices\n",
        "# print(x[1:]) # Starts from index 1 and all\n",
        "# print(x[1:3]) # Starts from index 1 but exclude index 3\n",
        "# print(x[::2]) # Skip every other element\n",
        "# print(x[::-1]) # Reverse order\n",
        "\n",
        "# # If you want to point specific indices\n",
        "# indices = tf.constant([0, 3])\n",
        "# x_ind = tf.gather(x, indices)\n",
        "# print(x_ind)\n",
        "\n",
        "\n",
        "# <Matrix>\n",
        "x = tf.constant([[1,2],\n",
        "                [3,4],\n",
        "                [5,6]])\n",
        "# print(x[0, :]) # Get all the elements in the 1st row. Also you can write this as (x[0])\n",
        "print(x[0:2, :]) # Specify the indices to get first 2 rows and all the elements in it\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RTFfTl2c5GDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nov 12th 2023 (a day off)\n",
        "<br>Reshaping  [Official Documentation](https://www.tensorflow.org/api_docs/python/tf/transpose)"
      ],
      "metadata": {
        "id": "1sZNcsg3jzs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x = tf.range(9) # A vector which has 9 elements\n",
        "# print(x)\n",
        "\n",
        "# x = tf.reshape(x, (3,3)) # reshape the vector to a 3*3 matrix\n",
        "# print(x)\n",
        "\n",
        "# x = tf.transpose(x, perm=[1,0]) # Transpose the matrix. perm for permutaion. [1, 0] means just to swap the axis\n",
        "# print(x)\n",
        "\n",
        "# For n > 2 dimensional tensors\n",
        "x = tf.constant([[[ 1,  2,  3],\n",
        "                  [ 4,  5,  6]],\n",
        "                 [[ 7,  8,  9],\n",
        "                  [10, 11, 12]]])\n",
        "print(x)\n",
        "\n",
        "x1 = tf.transpose(x) # default\n",
        "print(x1)\n",
        "print(\"-----------------\")\n",
        "x = tf.transpose(x, perm=[0,2,1])\n",
        "print(x)"
      ],
      "metadata": {
        "id": "C_YOX0Za_otw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nov 13th 2023 Chapter 3 [link](https://www.youtube.com/watch?v=pAhPiF3yiXI&list=PLhhyoLH6IjfxVOdVC1P1L5z5azs0XjMsb&index=3)"
      ],
      "metadata": {
        "id": "StwZYRzzD_9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the mnist data set\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "# This is how the printed value look like\n",
        "#(60000, 28, 28)\n",
        "# As we send them to NN, we need to flatten them so that we only have one long column with those feature values\n",
        "# Here is how we do it\n",
        "x_train = x_train.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "\n",
        "# -1 means to keep whatever the value is on that dimension, in this case 60,000.\n",
        "# Then flatten these two dimensions, 28*28\n",
        "# When we load the data, they are gonna be in numpy arrays, and float 64. So we minimize some of the computation with astype\n",
        "# And normalize the value by dividing by 255. This is because they are rather than being between 0 and 255, make it between 0 and 1 for faster training\n",
        "# Do the same thing for x_test\n",
        "\n",
        "# Play around w/o normlization to see if the result differes\n",
        "# x_train = x_train.reshape(-1, 28*28).astype(\"float32\")\n",
        "# x_test = x_test.reshape(-1, 28*28).astype(\"float32\")\n"
      ],
      "metadata": {
        "id": "1LcpJDIPD-bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential API (very convenient, not very flexible, meaning it only allows you to have one input mapped to one output )\n",
        "<br><br>[About from_logits=True](https://datascience.stackexchange.com/questions/73093/what-does-from-logits-true-do-in-sparsecategoricalcrossentropy-loss-function)\n"
      ],
      "metadata": {
        "id": "wsQ_Ij0WM0BA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Cross Entropy Loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#:~:text=Cross%2Dentropy%20loss%2C%20or%20log,diverges%20from%20the%20actual%20label.)\n",
        "\n",
        "What \"Sparse\" means? <br>\n",
        "It means that the labels, such the y_train labels, are just an integer for that correct label. E.g, if it's a digit of three, the y train o that specific example will just be three. <br>\n",
        "If you remove the \"Sparse\", then you need to have **one-hot encodings**.\n",
        "<br><br>\n",
        "Optimizers <br>shape and mold your model into its most accurate possible form by futzing with the weights. The loss function is the guide to the terrain(territory), telling the optimizer when it's moving in the right or wrong direction. Optimizers are related to model accuracy, a key component of AI/ML governance.\n",
        "<br><br>\n",
        " \"Batch size\" <br>refers to the number of training examples utilized in one iteration. In the context of the fit method in TensorFlow/Keras, the batch_size parameter determines the number of samples that will be used in each iteration to update the model's weights.\n",
        "<br><br>\n",
        "[Verbose](https://www.educba.com/keras-verbose/) – It will specify the mode of verbosity. <br>\n",
        "We are defining 0 for silent, one for the progress bar, and two for one line per epoch.\n",
        "\n",
        "<br>\n",
        "keras.Input(shape=(28*28)) <br>\n",
        "Lets you print(model.summary).\n",
        "<br>\n",
        "W/o keras.Input, you have to do it after the fit, meaning after you send something to the model to print the model summary"
      ],
      "metadata": {
        "id": "UUpjc4TGTVMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a model\n",
        "# Inside sequetial, we are gonna send a list which is layers\n",
        "# 512 abd such numbers are Nodes.\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(28*28)), # Specify the input\n",
        "        layers.Dense(512, activation='relu'), # first layer\n",
        "        layers.Dense(256, activation='relu'), # second layer\n",
        "        layers.Dense(10) # We want to map it to 10 nodes, one node for each digit. Since this is the output layer, we do not need activation function\n",
        "        # We are gonna use softmax on the output function but it is gonna be done inside the loss function\n",
        "        ]\n",
        ")\n",
        "\n",
        "# print(model.summary())\n",
        "# import sys\n",
        "# sys.exit()\n",
        "\n",
        "# this is to tell Keras how to configure the training part of our network\n",
        "model.compile(\n",
        "   loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Specify the loss function we want to use. Check the note above abt the argument.\n",
        "   # First it sends it into a softmax, and then it's gonna map it to the Crossentropy\n",
        "\n",
        "   optimizer=keras.optimizers.Adam(learning_rate=0.001), # Specify the optimizers. lr is for Learning Rate\n",
        "   metrics=[\"accuracy\"] # Keras tracks what is the running accuracy\n",
        ")\n",
        "\n",
        "#As you can view it that model.compile specifies the network configurations(designs),\n",
        "# model.fit, we specify more of the concrete training of THAT network\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2) # verbose=2 prints values after each epoch\n",
        "\n",
        "#After the training, we want to evaluate\n",
        "print(\"\\n Here is the evaluation!!!\\n\")\n",
        "model.evaluate(x_test, y_test, batch_size=32, verbose=2)"
      ],
      "metadata": {
        "id": "YElPBnD-M540"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Good tip for debugging\n",
        "# You can build a model this way\n",
        "# This is not the simplest way to write, but you can print the summary each time you add the layer for debugging\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(784)))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "# print(model.summary())\n",
        "model.add(layers.Dense(256, activation='relu', name='tomb_layer')) # you can set the nickname for a layer\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "# this is how you extract specific layer outputs which is useful for debugging also\n",
        "# first, overwrite the model\n",
        "\n",
        "# <Call layer with index>\n",
        "# model = keras.Model(inputs=model.inputs,\n",
        "#                     outputs=[model.layers[-2].output]) # if you do -1, it is the output layer a.k.a the last one. -2 means you get the one before that\n",
        "\n",
        "# # <Call layer with a nickname>\n",
        "# model = keras.Model(inputs=model.inputs,\n",
        "#                     outputs=[model.get_layer('tomb_layer').output]) # you can call the layer with the nick name instead of index\n",
        "\n",
        "# feature = model.predict(x_train)\n",
        "# print(feature.shape) # in this case, 60000 training data and 256 layers which we set as the second layer\n",
        "\n",
        "\n",
        "# <Call all layers with for loop>\n",
        "model = keras.Model(inputs=model.inputs,\n",
        "                    outputs=[layer.output for layer in model.layers])\n",
        "\n",
        "features = model.predict(x_train)\n",
        "\n",
        "for feature in features:\n",
        "  print(feature.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IOn_lWsAhtpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f88179-9382-4ab8-b0f2-089db30ea12e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 4s 2ms/step\n",
            "(60000, 10)\n",
            "1875/1875 [==============================] - 3s 2ms/step\n",
            "(60000, 512)\n",
            "(60000, 256)\n",
            "(60000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functional API** (a bit more flexible)\n",
        "<br>\n",
        "<br>\n",
        "SUGGESTIONS:<br>\n",
        "1, Try and see what accuracy you can get by increasing the model, training for longer etc..<br> You should be able to get over 98.1% on the test set!\n",
        "<br>\n",
        "<br>\n",
        "2, Try uing different optimizers than Adam, <br>e.g, Gradient Descent with Momentum\n",
        "Adagrad, and RMSprop\n",
        "<br>\n",
        "<br>\n",
        "3, Is there any difference if you remove the normalization of the data?\n"
      ],
      "metadata": {
        "id": "Mt3zEVmHi1zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nov 14th 2023\n",
        "\n",
        "inputs = keras.Input(shape=(784))\n",
        "x = layers.Dense(512, activation='relu', name='first_layer')(inputs) # Call inputs here\n",
        "x = layers.Dense(256, activation='relu', name='second_layer')(x) # send in the previous layer in this case, (x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# make a model\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# print(model.summary())\n",
        "# import sys\n",
        "# sys.exit()\n",
        "\n",
        "# Use the same model aobve except from_logits=False since we used softmax in the output layer\n",
        "# from_logits=False means default so you can only remove it inside the (), but keep ()\n",
        "model.compile(\n",
        "   loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "\n",
        "   optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "   metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\n",
        "\n",
        "print(\"\\n Here is the evaluation!!!\\n\")\n",
        "model.evaluate(x_test, y_test, batch_size=32, verbose=2)"
      ],
      "metadata": {
        "id": "RD8jH1GfUNMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional NN** [reference](https://youtu.be/WAciKiDP2bo?feature=shared)\n",
        "\n",
        "<br>\n",
        "Memo<br>\n",
        "------------\n",
        "\n",
        "<br>\n",
        "layers.Conv2D(32, 3, padding='valid', activation='relu')\n",
        "<br>-the first argument 32 means how many channels we want this convolutional layer to output. We got 3 channels first then we want it to be 32 channels <br>\n",
        "-The second argument 3 means we specifying the kernel size. Set a single integer automatically becomes (3,3) or 3*3 kernel size<br>\n",
        "-padding is that 'valid' is default. Use 'same' keeps the original value of shape, in this case (height 32, width 32)<br>\n",
        "Using 'valid' changes the value depeding on the kernel size. In this case it becomes 30*30 pixels\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "KzS9JVARgRDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0 # 255 is a pixel range, and normalize here make it between 0 and 1\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Let's start with Sequential model then make it more advanced later\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(32, 32, 3)), # 32 for Height, 32 for Width, 3 for RGB colour channels\n",
        "        layers.Conv2D(32, 3, padding='valid', activation='relu'), # this line is explained above the note\n",
        "        layers.MaxPooling2D(pool_size=(2,2)), # 2*2 means half the input size\n",
        "        layers.Conv2D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(128, 3, activation='relu'),\n",
        "        layers.Flatten(), # from here the preparation for the actual output\n",
        "        layers.Dense(64, activation='relu'), # 64 nodes which is an intermdediate value (can vary)\n",
        "        layers.Dense(10), # this is the actual output\n",
        "    ]\n",
        ")\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    metrics=['accuracy'],\n",
        "\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6Gf5H0PiuSC",
        "outputId": "79419e7c-7437-49f5-bd7c-c8035ff9ff0f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_7 (Conv2D)           (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 15, 15, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 6, 6, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 4, 4, 128)         73856     \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 2048)              0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 64)                131136    \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 225034 (879.04 KB)\n",
            "Trainable params: 225034 (879.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}