{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPAkBuwSpSY8AqmWymLMN5J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TombCrawler/nihonAI/blob/main/TensorTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "referred"
      ],
      "metadata": {
        "id": "-sjlH-LzjC3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "id": "cZCatou1E6M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "78YW6KByi_CL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.datasets import cifar10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "initialization of Tensors"
      ],
      "metadata": {
        "id": "9X3EHseUjovb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaler\n",
        "x = tf.constant(4, shape=(1,1), dtype=tf.float32)\n",
        "\n",
        "# Matrix, default 2 * 3 meaning shape=(2,3)\n",
        "x = tf.constant([[1,2,3], [4,5,6]])\n",
        "\n",
        "x = tf.ones((3,3))\n",
        "x = tf.zeros((2,3))\n",
        "x = tf.eye(3) # I for the indentity\n",
        "x = tf.random.normal((3,3), mean=0, stddev=1) # stddv for standard deviation\n",
        "x = tf.random.uniform((1,3), minval=0, maxval=1)\n",
        "x = tf.range(start=1, limit=10, delta=2) # delta is steps.\n",
        "x = tf.cast(x, dtype=tf.float64)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "QkihwZgNj7mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematical Operations\n"
      ],
      "metadata": {
        "id": "DLzgK999jsTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([1,2,3])\n",
        "y = tf.constant([9,8,7])\n",
        "\n",
        "z = tf.add(x,y)\n",
        "# z = x + y\n",
        "\n",
        "z = tf.subtract(x, y)\n",
        "\n",
        "z = tf.divide(x, y)\n",
        "z = x/y\n",
        "\n",
        "z = tf.multiply(x, y)\n",
        "\n",
        "z = tf.tensordot(x, y, axes=1)\n",
        "z = tf.reduce_sum(x*y, axis=0)\n",
        "\n",
        "z = x ** 5\n",
        "\n",
        "x = tf.random.normal((2,3))\n",
        "y = tf.random.normal((3,4))\n",
        "z = tf.matmul(x, y)\n",
        "print(z)\n",
        "z = x @ y\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AU130I9ew4d9",
        "outputId": "7919c1b6-183e-43d2-d94f-66d9af02b012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.0623768   1.777854   -0.15115848 -0.57498646]\n",
            " [-1.8350759   4.2908087  -0.4893546  -0.20726681]], shape=(2, 4), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.0623768   1.777854   -0.15115848 -0.57498646]\n",
            " [-1.8350759   4.2908087  -0.4893546  -0.20726681]], shape=(2, 4), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing"
      ],
      "metadata": {
        "id": "12BEgXErjxTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # <Vector>\n",
        "# x = tf.constant([6, 7, 8, 1, 3, 9, 2, 5])\n",
        "# print(x[:]) # Print all indices\n",
        "# print(x[1:]) # Starts from index 1 and all\n",
        "# print(x[1:3]) # Starts from index 1 but exclude index 3\n",
        "# print(x[::2]) # Skip every other element\n",
        "# print(x[::-1]) # Reverse order\n",
        "\n",
        "# # If you want to point specific indices\n",
        "# indices = tf.constant([0, 3])\n",
        "# x_ind = tf.gather(x, indices)\n",
        "# print(x_ind)\n",
        "\n",
        "\n",
        "# <Matrix>\n",
        "x = tf.constant([[1,2],\n",
        "                [3,4],\n",
        "                [5,6]])\n",
        "# print(x[0, :]) # Get all the elements in the 1st row. Also you can write this as (x[0])\n",
        "print(x[0:2, :]) # Specify the indices to get first 2 rows and all the elements in it\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RTFfTl2c5GDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nov 12th 2023 (a day off)\n",
        "<br>Reshaping  [Official Documentation](https://www.tensorflow.org/api_docs/python/tf/transpose)"
      ],
      "metadata": {
        "id": "1sZNcsg3jzs-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x = tf.range(9) # A vector which has 9 elements\n",
        "# print(x)\n",
        "\n",
        "# x = tf.reshape(x, (3,3)) # reshape the vector to a 3*3 matrix\n",
        "# print(x)\n",
        "\n",
        "# x = tf.transpose(x, perm=[1,0]) # Transpose the matrix. perm for permutaion. [1, 0] means just to swap the axis\n",
        "# print(x)\n",
        "\n",
        "# For n > 2 dimensional tensors\n",
        "x = tf.constant([[[ 1,  2,  3],\n",
        "                  [ 4,  5,  6]],\n",
        "                 [[ 7,  8,  9],\n",
        "                  [10, 11, 12]]])\n",
        "print(x)\n",
        "\n",
        "x1 = tf.transpose(x) # default\n",
        "print(x1)\n",
        "print(\"-----------------\")\n",
        "x = tf.transpose(x, perm=[0,2,1])\n",
        "print(x)"
      ],
      "metadata": {
        "id": "C_YOX0Za_otw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nov 13th 2023 Chapter 3 [link](https://www.youtube.com/watch?v=pAhPiF3yiXI&list=PLhhyoLH6IjfxVOdVC1P1L5z5azs0XjMsb&index=3)"
      ],
      "metadata": {
        "id": "StwZYRzzD_9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the mnist data set\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(x_train.shape)\n",
        "# This is how the printed value look like\n",
        "#(60000, 28, 28)\n",
        "# As we send them to NN, we need to flatten them so that we only have one long column with those feature values\n",
        "# Here is how we do it\n",
        "x_train = x_train.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "\n",
        "# -1 means to keep whatever the value is on that dimension, in this case 60,000.\n",
        "# Then flatten these two dimensions, 28*28\n",
        "# When we load the data, they are gonna be in numpy arrays, and float 64. So we minimize some of the computation with\n",
        "# astype (You can also convert one type to another type with the astype() method.)\n",
        "# And normalize the value by dividing by 255. This is because they are rather than being between 0 and 255, make it between 0 and 1 for faster training\n",
        "# Do the same thing for x_test\n",
        "\n",
        "# Play around w/o normlization to see if the result differes\n",
        "# x_train = x_train.reshape(-1, 28*28).astype(\"float32\")\n",
        "# x_test = x_test.reshape(-1, 28*28).astype(\"float32\")\n"
      ],
      "metadata": {
        "id": "1LcpJDIPD-bW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c875fd5-7a2c-4785-d7aa-6530ed1abdc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sequential API** (very convenient, not very flexible, meaning it only allows you to have one input mapped to one output )\n",
        "<br><br>[About from_logits=True](https://datascience.stackexchange.com/questions/73093/what-does-from-logits-true-do-in-sparsecategoricalcrossentropy-loss-function)\n"
      ],
      "metadata": {
        "id": "wsQ_Ij0WM0BA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Cross Entropy Loss](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#:~:text=Cross%2Dentropy%20loss%2C%20or%20log,diverges%20from%20the%20actual%20label.)\n",
        "\n",
        "What \"Sparse\" means? <br>\n",
        "It means that the labels, such the y_train labels, are just an integer for that correct label. E.g, if it's a digit of three, the y train o that specific example will just be three. <br>\n",
        "If you remove the \"Sparse\", then you need to have **one-hot encodings**.\n",
        "<br><br>\n",
        "Optimizers <br>shape and mold your model into its most accurate possible form by futzing with the weights. The loss function is the guide to the terrain(territory), telling the optimizer when it's moving in the right or wrong direction. Optimizers are related to model accuracy, a key component of AI/ML governance.\n",
        "<br><br>\n",
        " \"Batch size\" <br>refers to the number of training examples utilized in one iteration. In the context of the fit method in TensorFlow/Keras, the batch_size parameter determines the number of samples that will be used in each iteration to update the model's weights.\n",
        "<br><br>\n",
        "[Verbose](https://www.educba.com/keras-verbose/) â€“ It will specify the mode of verbosity. <br>\n",
        "We are defining 0 for silent, one for the progress bar, and two for one line per epoch.\n",
        "\n",
        "<br>\n",
        "keras.Input(shape=(28*28)) <br>\n",
        "Lets you print(model.summary).\n",
        "<br>\n",
        "W/o keras.Input, you have to do it after the fit, meaning after you send something to the model to print the model summary"
      ],
      "metadata": {
        "id": "UUpjc4TGTVMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a model\n",
        "# Inside sequetial, we are gonna send a list which is layers\n",
        "# 512 abd such numbers are Nodes.\n",
        "# model = keras.Sequential(\n",
        "#     [\n",
        "#         keras.Input(shape=(28*28)), # Specify the input\n",
        "#         layers.Dense(512, activation='relu'), # first layer\n",
        "#         layers.Dense(256, activation='relu'), # second layer\n",
        "#         layers.Dense(10) # We want to map it to 10 nodes, one node for each digit. Since this is the output layer, we do not need activation function\n",
        "#         # We are gonna use softmax on the output function but it is gonna be done inside the loss function\n",
        "#         ]\n",
        "# )\n",
        "\n",
        "# print(model.summary())\n",
        "# import sys\n",
        "# sys.exit()\n",
        "\n",
        "\n",
        "# this is to tell Keras how to configure the training part of our network\n",
        "model.compile(\n",
        "   loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True), # Specify the loss function we want to use. Check the note above abt the argument.\n",
        "   # First it sends it into a softmax, and then it's gonna map it to the Crossentropy\n",
        "\n",
        "   optimizer=keras.optimizers.Adam(learning_rate=0.001), # Specify the optimizers. lr is for Learning Rate\n",
        "   metrics=[\"accuracy\"] # Keras tracks what is the running accuracy\n",
        ")\n",
        "\n",
        "#As you can view it that model.compile specifies the network configurations(designs),\n",
        "# model.fit, we specify more of the concrete training of THAT network\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2) # verbose=2 prints values after each epoch\n",
        "\n",
        "#After the training, we want to evaluate\n",
        "print(\"\\n Here is the evaluation!!!\\n\")\n",
        "model.evaluate(x_test, y_test, batch_size=32, verbose=2)"
      ],
      "metadata": {
        "id": "YElPBnD-M540"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Conv for MNIST (Nov 15th 2023)\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# !!!THIS IS DIFFERENT!!!! than the FC.\n",
        "# FC looks this\n",
        "# x_train = x_train.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "# x_test = x_test.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "\n",
        "# Change the dimension to the reshape when you want to use Conv\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "print(x_train.shape)\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "\n",
        "\n",
        "\n",
        "#_oA corrected one\n",
        "def my_model():\n",
        "    inputs = keras.Input(shape=(28, 28, 1)) # if the image is gray scale, set the channel as 1\n",
        "    x = layers.Conv2D(32, 3)(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.MaxPooling2D()(x)\n",
        "    x = layers.Conv2D(32, 3, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Conv2D(64, 3)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = keras.activations.relu(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)  # Assuming 10 classes for classification\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = my_model()\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\n",
        "\n",
        "print(\"\\nHere is the evaluation!!!\\n\")\n",
        "model.evaluate(x_test, y_test, batch_size=32, verbose=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "vbiTld6OjdPr",
        "outputId": "58548f9e-3cda-4508-f331-3bf721704d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "(60000, 28, 28, 1)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 - 25s - loss: 0.1145 - accuracy: 0.9658 - 25s/epoch - 13ms/step\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c900a63c0aa3>\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nHere is the evaluation!!!\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1781\u001b[0m                         ):\n\u001b[1;32m   1782\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1783\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1784\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    868\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1262\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1263\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mflat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;34m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    253\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1479\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1480\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[0;32m---> 60\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     61\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Good tip for debugging\n",
        "# You can build a model this way\n",
        "# This is not the simplest way to write, but you can print the summary each time you add the layer for debugging\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(784)))\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "# print(model.summary())\n",
        "model.add(layers.Dense(256, activation='relu', name='tomb_layer')) # you can set the nickname for a layer\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "# this is how you extract specific layer outputs which is useful for debugging also\n",
        "# first, overwrite the model\n",
        "\n",
        "# <Call layer with index>\n",
        "# model = keras.Model(inputs=model.inputs,\n",
        "#                     outputs=[model.layers[-2].output]) # if you do -1, it is the output layer a.k.a the last one. -2 means you get the one before that\n",
        "\n",
        "# # <Call layer with a nickname>\n",
        "# model = keras.Model(inputs=model.inputs,\n",
        "#                     outputs=[model.get_layer('tomb_layer').output]) # you can call the layer with the nick name instead of index\n",
        "\n",
        "# feature = model.predict(x_train)\n",
        "# print(feature.shape) # in this case, 60000 training data and 256 layers which we set as the second layer\n",
        "\n",
        "\n",
        "# <Call all layers with for loop>\n",
        "model = keras.Model(inputs=model.inputs,\n",
        "                    outputs=[layer.output for layer in model.layers])\n",
        "\n",
        "features = model.predict(x_train)\n",
        "\n",
        "for feature in features:\n",
        "  print(feature.shape)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IOn_lWsAhtpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5f88179-9382-4ab8-b0f2-089db30ea12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 4s 2ms/step\n",
            "(60000, 10)\n",
            "1875/1875 [==============================] - 3s 2ms/step\n",
            "(60000, 512)\n",
            "(60000, 256)\n",
            "(60000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functional API** (a bit more flexible)\n",
        "<br>\n",
        "<br>\n",
        "SUGGESTIONS:<br>\n",
        "1, Try and see what accuracy you can get by increasing the model, training for longer etc..<br> You should be able to get over 98.1% on the test set!\n",
        "<br>\n",
        "<br>\n",
        "2, Try uing different optimizers than Adam, <br>e.g, Gradient Descent with Momentum\n",
        "Adagrad, and RMSprop\n",
        "<br>\n",
        "<br>\n",
        "3, Is there any difference if you remove the normalization of the data?\n"
      ],
      "metadata": {
        "id": "Mt3zEVmHi1zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nov 14th 2023\n",
        "\n",
        "inputs = keras.Input(shape=(784))\n",
        "x = layers.Dense(512, activation='relu', name='first_layer')(inputs) # Call and send inputs to this layer\n",
        "x = layers.Dense(256, activation='relu', name='second_layer')(x) # send in the previous layer in this case, (x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "# make a model\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# print(model.summary())\n",
        "# import sys\n",
        "# sys.exit()\n",
        "\n",
        "# Use the same model aobve except from_logits=False since we used softmax in the output layer\n",
        "# from_logits=False means default so you can only remove it inside the (), but keep ()\n",
        "model.compile(\n",
        "   loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "\n",
        "   optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "   metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, verbose=2)\n",
        "\n",
        "print(\"\\n Here is the evaluation!!!\\n\")\n",
        "model.evaluate(x_test, y_test, batch_size=32, verbose=2)"
      ],
      "metadata": {
        "id": "RD8jH1GfUNMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convolutional NN** [reference](https://youtu.be/WAciKiDP2bo?feature=shared)\n",
        "\n",
        "<br>\n",
        "Memo<br>\n",
        "------------\n",
        "\n",
        "<br>\n",
        "layers.Conv2D(32, 3, padding='valid', activation='relu')\n",
        "<br>-the first argument 32 means how many channels we want this convolutional layer to output. We got 3 channels first then we want it to be 32 channels <br>\n",
        "-The second argument 3 means we specifying the kernel size. Set a single integer automatically becomes (3,3) or 3*3 kernel size<br>\n",
        "-padding is that 'valid' is default. Use 'same' keeps the original value of shape, in this case (height 32, width 32)<br>\n",
        "Using 'valid' changes the value depeding on the kernel size. In this case it becomes 30*30 pixels\n",
        "<br>\n",
        "<br>\n",
        "This models's total parameters will be \"Total params: 225034\" which is considered to be very small as AlexNet (the first convolutional NN) had about 60 Million params.\n"
      ],
      "metadata": {
        "id": "KzS9JVARgRDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0 # 255 is a pixel range, and normalize here make it between 0 and 1\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Let's start with\n",
        "#a Sequential API model then make it more advanced later\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(32, 32, 3)), # 32 for Height, 32 for Width, 3 for RGB colour channels\n",
        "        layers.Conv2D(32, 3, padding='valid', activation='relu'), # this line is explained above the note\n",
        "        layers.MaxPooling2D(pool_size=(2,2)), # 2*2 means half the input size, you can change it whatever size you want but 2*2 is default\n",
        "        layers.Conv2D(64, 3, activation='relu'),\n",
        "        layers.MaxPooling2D(), # pool_size is not specified because 2*2 is default\n",
        "        layers.Conv2D(128, 3, activation='relu'),\n",
        "        layers.Flatten(), # from here the preparation for the actual output\n",
        "        layers.Dense(64, activation='relu'), # 64 nodes which is an intermdediate value (can vary)\n",
        "        layers.Dense(10), # this is the actual output\n",
        "    ]\n",
        ")\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "id": "N6Gf5H0PiuSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functional API** / Convolutional NN @10:24<br>\n",
        "\n",
        "We do not use an activation function in the first Conv2D layer, because if we're using batch norm we want to <br>\n",
        "first, send it through the convolutional area, <br>\n",
        "Second, then through the batch norm,<br>\n",
        "Third, then send it through th activation function\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Dense layer <br>(also known as a fully connected layer) is a type of layer that connects each neuron or node in one layer to every neuron in the next layer. It's a fundamental building block in artificial neural networks.\n",
        "<br>\n",
        "-------------<br>\n",
        "Suggestions<br>\n",
        "\n",
        "1, What accuracy can you get on the test set by training longer, increasing the model size, changing kernel sizes etc...?\n",
        "<br>\n",
        "2, In the last video, we trained a FC(Fully Connected) onn MNIST, what can you get by using a conv net instead?"
      ],
      "metadata": {
        "id": "4G5D691hS5Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functional API / Convolutional NN @10:24\n",
        "# this is an advanced way\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0 # 255 is a pixel range, and normalize here make it between 0 and 1\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "def my_model():\n",
        "  inputs = keras.Input(shape=(32, 32, 3))\n",
        "  x = layers.Conv2D(32, 3)(inputs) # avoid use activation function here due to Batch norm after this\n",
        "  x = layers.BatchNormalization()(x) # Check the TS&Study note where Andrew Ng video watched\n",
        "  x = keras.activations.relu(x) # this is how you send data through avtivation function after batch norm\n",
        "  x = layers.MaxPooling2D()(x)\n",
        "  x = layers.Conv2D(64, 3, padding='same')(x) # 64 channels.  the kernel size of 5, padding for same just for fun.\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Conv2D(128, 3)(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Flatten()(x) # W/o this, you get an invalid argumnt error\n",
        "  x = layers.Dense(64, activation='relu')(x)\n",
        "  outputs = layers.Dense(10)(x)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "model = my_model()\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=3, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=32, verbose=2)"
      ],
      "metadata": {
        "id": "jSxbEt3IKtez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nov16th 2023 <br>\n",
        "Chapter 5[Reference](https://youtu.be/kJSUq1PLmWg?list=PLhhyoLH6IjfxVOdVC1P1L5z5azs0XjMsb)\n",
        "<br>\n",
        "**L2 Regularization and Dropout**<br>\n",
        "\n",
        "Here, we are using 3 regularization method. <br>\n",
        "1, Regularizer. 2, Dropout. 3, Batch Norm (This can be considered as regularizing method)"
      ],
      "metadata": {
        "id": "jFz3ukTVNvN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the code from Conv Functional API\n",
        "from tensorflow.keras import layers, regularizers # Import regularizers from keras\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "def my_model():\n",
        "  inputs = keras.Input(shape=(32, 32, 3))\n",
        "  x = layers.Conv2D(\n",
        "      32, 3, padding='same', kernel_regularizer=regularizers.l2(0.01),)(inputs) # add regularizers for each layer. padding for same for simplicity\n",
        "  x = layers.BatchNormalization()(x) # the purposes of Batch norm is more to normalize data with faster training, but also has regularinzing effect\n",
        "  x = keras.activations.relu(x) # this is how you send data through avtivation function after batch norm\n",
        "  x = layers.MaxPooling2D()(x)\n",
        "  x = layers.Conv2D(\n",
        "      64, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(x) # 64 channels.  the kernel size of 5, padding for same just for fun.\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Conv2D(128, 3,\n",
        "                    padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Flatten()(x) # W/o this, you get an invalid argumnt error\n",
        "  x = layers.Dense(\n",
        "      64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x) # add regularizers to a FC(fully connected layer a.k.a dense)\n",
        "  x = layers.Dropout(0.5)(x) # set dropout which drops 0.5 of conncections btween the FC above and the output layer\n",
        "  outputs = layers.Dense(10)(x)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "model = my_model()\n",
        "\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=5, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=32, verbose=2)"
      ],
      "metadata": {
        "id": "GHi1Q9VVORSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chapter 6 [reference](https://youtu.be/Ogvd787uJO8?feature=shared)<br>\n",
        "\n",
        "**RNNs, GRUs, LSTMs and Bidirectionality**\n",
        "<br>\n",
        "* You would not use Sequence models to handle images but use Conv model, or RNNs still work ok. <br>\n",
        "--------\n",
        "<br>\n",
        "model.add(keras.Input(shape=(None, 28)))<br><br>\n",
        " # We set None here because we do not have to have a specific number of time step in this dimension instead we have 28 which means 28 pixels in each step\n",
        "<br>-------<br>\n",
        "model.add(<br>\n",
        "      layers.SimpleRNN(512, return_sequences=True, activation='relu')\n",
        "    )<br><br>\n",
        "    # 512 nodes as random number.<br>Return_sequences=True means it's returning the output each time step. In that way we can **stack** multiple RNN layers on top of each other. Output from this RNN is 512 nodes,then return sequences, i.e. it outputs 512 for each time meaning 28 times in this case.  <br>\n",
        "    We do not have return_sequences=True for the last sipleRNN bfore the output layer because we take the last layer Dense on top of it, meaning 10 output nodes.\n",
        "    "
      ],
      "metadata": {
        "id": "m_MlXHZNWVjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.0 # Use astype to convert the default \"float64\" to save on some computaion as float 32\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(None, 28))) # explanation in the text\n",
        "model.add(\n",
        "    layers.SimpleRNN(512, return_sequences=True, activation='relu') # explanation in the text\n",
        "    )\n",
        "model.add(layers.SimpleRNN(512, activation='relu')) #\n",
        "model.add(layers.Dense(10)) # output layer\n",
        "\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4kb_tH_We-d",
        "outputId": "94c4bca2-06d1-4cd5-e765-9b12fbb23d25"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn (SimpleRNN)      (None, None, 512)         276992    \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 512)               524800    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 806922 (3.08 MB)\n",
            "Trainable params: 806922 (3.08 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    }
  ]
}